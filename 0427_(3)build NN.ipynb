{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69THVqhn2oiu"
      },
      "source": [
        "### 신경망 모델 구성하기\n",
        "* 신경망: layer와 module로 구성 \n",
        "* Pytorch의 모든 모듈은 nn.Module의 하위클래스  \n",
        "  * 신경망은 다른 모듈(layer)로 구성된 모듈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vHyRRkT2bB7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnX3Al_g3A5R",
        "outputId": "30473dfb-b27f-4011-d971-a2be72e89c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "#학습을 위한 장치\n",
        "# torch.cuda를 사용할 수 있는지 확인하고 그렇지 않으면 cpu사용\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6csd_3J73SM4"
      },
      "source": [
        "### 클래스 정의하기\n",
        "* 신경망 모델을 nn.Module의 하위클래스로 정의하고,  ```__init__```\n",
        "에서 신경망 계층들을 초기화  \n",
        "\n",
        "* nn.Module를 상속받은 모든 클래스는 forward 메소드에 입력 데이터에 대한 연산들을 구현함  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU-IWOvG3Cj7"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(NeuralNetwork, self).__init__()\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear_relu_stack = nn.Sequential(\n",
        "        nn.Linear(28*28, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512, 512),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(512,10),\n",
        "    )\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.flatten(x)\n",
        "    logits = self.linear_relu_stack(x)\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MW4yLE2E3CRh",
        "outputId": "a0929663-44fc-4c6b-ae35-b760e3345bfa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "model = NeuralNetwork().to(device)\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vqMjhLoqxmd",
        "outputId": "5f01a627-12ea-4f93-a4c2-6543fcd93898"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "Predicted class: tensor([5], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "X = torch.rand(1, 28, 28, device=device)\n",
        "print(X.shape)\n",
        "logits = model(X)\n",
        "pred_probab = nn.Softmax(dim=1)(logits)\n",
        "y_pred = pred_probab.argmax(1)\n",
        "print(f\"Predicted class: {y_pred}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcXTjsnHrLip"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22juO43KrD0I"
      },
      "source": [
        "### 모델 계층(layer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "95kpyG5EpbqQ",
        "outputId": "11012571-18db-4cc9-d545-150c3a60f853"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "input_image = torch.rand(3, 28, 28)\n",
        "print(input_image.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TdTsSfjrydc"
      },
      "source": [
        "nn.Flatten  \n",
        "* nn.Flatten계층을 초기화하여 각 28*28의 2D이미지를 784 픽셀 값을 갖는 연속된 배열로 변환함."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8D2e-5Gpbep",
        "outputId": "505df047-0db1-48ec-85e9-20bd616f0f1c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 784])\n"
          ]
        }
      ],
      "source": [
        "flatten = nn.Flatten()\n",
        "flat_image = flatten(input_image)\n",
        "print(flat_image.size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDW95X2jpbIy"
      },
      "outputs": [],
      "source": [
        "class Flatten(torch.nn.Module):\n",
        "  def forward(self, x):\n",
        "    batch_size = x.shape[0]\n",
        "    return x.view(batch_size, -1)\n",
        "\n",
        "# f = Flatten() "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KL-wXT0lyW4F"
      },
      "source": [
        "nn.Linear  \n",
        "* 선형 계층은 저장된 가중치와 편향을 사용하여 입력에 선형변형(linear transformation)을 적용하는 모듈"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuXbqdCZyYe8",
        "outputId": "f4e9cdd7-7f9b-4933-b50e-b1780823139b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([3, 20])\n"
          ]
        }
      ],
      "source": [
        "layer1 = nn.Linear(in_features=28*28, out_features=20)\n",
        "hidden1 = layer1(flat_image) #nn.Linear에 위에서 flatten한 image를 넣어줌\n",
        "print(hidden1.size())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFKVVPtHzOjh"
      },
      "source": [
        "nn.ReLU\n",
        "* activation : model input과 output사이의 복잡한 관계(mapping)를 만듦.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWBpR47zyYQ5",
        "outputId": "c37acfa0-8a96-41a3-d740-e6b2cad9ca10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before ReLU : tensor([[-0.0783,  0.4267, -0.1952,  0.0305,  0.3683,  0.1203, -0.3205,  0.2006,\n",
            "         -0.2462, -0.0437, -0.2835, -0.1353, -0.1426,  0.5756,  0.0881, -0.0827,\n",
            "          0.4027,  1.1206, -0.0272,  0.6430],\n",
            "        [-0.2887,  0.1454, -0.3945,  0.1720,  0.3197, -0.1711, -0.4305,  0.3609,\n",
            "         -0.0222, -0.0218, -0.0847, -0.3665,  0.0561,  0.0454,  0.4120, -0.1452,\n",
            "          0.4667,  1.0118,  0.1075,  0.1443],\n",
            "        [-0.2394,  0.5380, -0.3037, -0.2443,  0.4208,  0.0624, -0.1442,  0.5754,\n",
            "         -0.1836, -0.0768, -0.3601, -0.1608, -0.0195,  0.4727,  0.2943,  0.0565,\n",
            "          0.1974,  0.6396,  0.3220,  0.3145]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "After ReLU : tensor([[0.0000, 0.4267, 0.0000, 0.0305, 0.3683, 0.1203, 0.0000, 0.2006, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.5756, 0.0881, 0.0000, 0.4027, 1.1206,\n",
            "         0.0000, 0.6430],\n",
            "        [0.0000, 0.1454, 0.0000, 0.1720, 0.3197, 0.0000, 0.0000, 0.3609, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0561, 0.0454, 0.4120, 0.0000, 0.4667, 1.0118,\n",
            "         0.1075, 0.1443],\n",
            "        [0.0000, 0.5380, 0.0000, 0.0000, 0.4208, 0.0624, 0.0000, 0.5754, 0.0000,\n",
            "         0.0000, 0.0000, 0.0000, 0.0000, 0.4727, 0.2943, 0.0565, 0.1974, 0.6396,\n",
            "         0.3220, 0.3145]], grad_fn=<ReluBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(f\"Before ReLU : {hidden1}\\n\")\n",
        "hidden2 = nn.ReLU()(hidden1)\n",
        "print(f\"After ReLU : {hidden2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hzJedSGWzotb"
      },
      "source": [
        "nn.Sequential\n",
        "* nn.Sequential은 순서를 갖는 모듈의 컨테이너  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vwgc5NCrzn4-"
      },
      "outputs": [],
      "source": [
        "seq_modules = nn.Sequential(\n",
        "    flatten,\n",
        "    layer1, \n",
        "    nn.ReLU(),\n",
        "    nn.Linear(20,10)\n",
        ")\n",
        "input_image = torch.rand(3,28,28)\n",
        "logits = seq_modules(input_image)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnrop_mpz6Xp",
        "outputId": "98051af1-a691-49f8-fdda-475c2145cd69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.3423,  0.0803,  0.3334, -0.3385, -0.1712,  0.0085,  0.3474, -0.1136,\n",
            "         -0.1698, -0.0052],\n",
            "        [-0.2861,  0.0809,  0.2846, -0.3443, -0.3039,  0.0883,  0.3600, -0.2723,\n",
            "         -0.2878,  0.1397],\n",
            "        [-0.3228,  0.0555,  0.3610, -0.4039, -0.2651,  0.0473,  0.3621, -0.1605,\n",
            "         -0.3114,  0.0192]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "print(logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfO7437f1L31"
      },
      "source": [
        "nn.Softmax \n",
        "* 신경망의 마지막 선형 계층은 nn.Softmax모듈에 전달될 logits를 반환함.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b28AzQVU1MJn",
        "outputId": "c9a9e2d0-0ea7-4a96-f54f-6027e7ad39d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.0717, 0.1095, 0.1410, 0.0720, 0.0851, 0.1019, 0.1430, 0.0902, 0.0852,\n",
            "         0.1005],\n",
            "        [0.0767, 0.1107, 0.1357, 0.0723, 0.0753, 0.1115, 0.1463, 0.0777, 0.0765,\n",
            "         0.1174],\n",
            "        [0.0744, 0.1085, 0.1473, 0.0686, 0.0788, 0.1077, 0.1475, 0.0875, 0.0752,\n",
            "         0.1047]], grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "softmax = nn.Softmax(dim=1)\n",
        "pred_probab = softmax(logits)\n",
        "print(pred_probab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGeH7Q7p3L-a"
      },
      "source": [
        "모델 매개변수"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HWeqHpOk3KnN",
        "outputId": "3887a5e6-7b48-470a-cfac-5fb99eadead8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model structure : NeuralNetwork(\n",
            "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
            "  (linear_relu_stack): Sequential(\n",
            "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
            "  )\n",
            ")\n",
            "\n",
            "Layer: linear_relu_stack.0.weight | size : torch.Size([512, 784]) | Values : tensor([[-0.0210, -0.0226,  0.0009,  ..., -0.0126,  0.0067,  0.0284],\n",
            "        [ 0.0115, -0.0012,  0.0132,  ...,  0.0129,  0.0003, -0.0202]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.0.bias | size : torch.Size([512]) | Values : tensor([-0.0078, -0.0187], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.2.weight | size : torch.Size([512, 512]) | Values : tensor([[ 0.0405,  0.0124,  0.0432,  ...,  0.0397,  0.0048,  0.0182],\n",
            "        [-0.0035,  0.0172,  0.0102,  ..., -0.0415,  0.0298, -0.0207]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.2.bias | size : torch.Size([512]) | Values : tensor([-0.0090, -0.0346], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.4.weight | size : torch.Size([10, 512]) | Values : tensor([[ 0.0195, -0.0137, -0.0159,  ...,  0.0138, -0.0080,  0.0098],\n",
            "        [-0.0104,  0.0253,  0.0397,  ...,  0.0333,  0.0175, -0.0154]],\n",
            "       device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n",
            "Layer: linear_relu_stack.4.bias | size : torch.Size([10]) | Values : tensor([-0.0151, -0.0294], device='cuda:0', grad_fn=<SliceBackward0>)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Model structure : {model}\\n\")\n",
        "\n",
        "for name, param in model.named_parameters():\n",
        "  print(f\"Layer: {name} | size : {param.size()} | Values : {param[:2]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SoEWMaWpdfU"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEt3O8x0SL_e"
      },
      "source": [
        "###* convolution block을 구현\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNTEmjvEQ9X5"
      },
      "outputs": [],
      "source": [
        "import torch \n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6QVKDncok5i"
      },
      "source": [
        "###1. PyTorch 모델의 기본 구조 \n",
        "* PyTorch 모델로 쓰기 위해선 다음 두 가지 조건을 따라야한다. 내장된 모델(nn.Linear등)도 이를 만족한다. \n",
        "\n",
        "1. torch.nn.Module을 상속해야한다. \n",
        " * interitance: 상속; 어떤 클래스를 만들 때 다른 클래스의 기능을 그대로 가지고오는 것.  \n",
        "\n",
        "2. __init()__과 forward()를 override 해야한다. \n",
        " * override: 재정의; torch.nn.Module(부모클래스)에서 정의한 메소드를 자식클래스에서 변경하는 것. \n",
        " * __init()__에서는 모델에서 사용될 module(nn.Linear, nn.Conv2d), activation function(nn.functional.relu, nn.functional.sigmoid)등을 정의한다. \n",
        " * forward()에서는 모델에서 실행되어야하는 계산을 정의한다. backward 계산은 backward()를 이용하면 PyTorch가 알아서 해주니까 forward()만 정의해주면 된다. input을 넣어서 어떤 계산을 진행하하여 output이 나올지를 정의해준다고 이해하면 됨. \n",
        "\n",
        "\n",
        "### 2. nn.Module \n",
        "* PyTorch의 nn 라이브러리는 Neural Network의 모든 것을 포괄하는 모든 신경망 모델의 Base Class이다.  \n",
        "다른 말로, 모든 신경망 모델은 nn.Module의 subclass라고 할 수 있다. \n",
        "nn.Module을 상속한 subclass가 신경망 모델로 사용되기 위해선 앞서 소개한 두 메소드를 override 해야한다. \n",
        "\n",
        " * __init__(self): initialize; 내가 사용하고 싶은, 내 신경망 모델에 사용될 구성품들을 정의 및 초기화 하는 메소드이다. \n",
        " * forward(self, x): specify the connections;  이닛에서 정의된 구성품들을 연결하는 메소드이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9IiCxySRST0k"
      },
      "outputs": [],
      "source": [
        "class ConvBlock(nn.Module):\n",
        "'''\n",
        "ConvBlock은 nn.Module을 상속받는다.\n",
        "\n",
        "모듈이란, 한개 이상의 레이어가 모여서 구성된 것을 말함.\n",
        "\n",
        "pytorch의 layer를 사용하여 모델 build를 간편하게 하기 위해서는 위에 같이 nn.Module를 상속받고, \n",
        "이를 초기화함으로써, nn.Module에서 상속받는 특성들을 초기화해주는 것이 필요하다.\n",
        "'''\n",
        "\n",
        "  def __init__(self, kernel_size, stride, padding, pool, pool_stride):\n",
        "    super().__init__()   \n",
        "    #super로 기반클래스(부모클래스)를 초기화 -> 기반클래스의 속성을 subclass가 받아오도록 함.\n",
        "    #초기화 하지 않으면, 부모클래스의 속성을 사용할 수 없음\n",
        "\n",
        "    '''__init__() 를 override(재정의)해야 함.  \n",
        "    -> init에서는 모델에 사용될 module(nn.Linear, nn.Conv2d),activation function(nn.functional.relu) 등을 정의\n",
        "\n",
        "    '''\n",
        "\n",
        "    self.kernel_size = kernel_size\n",
        "    self.padding = padding\n",
        "    self.pool = pool\n",
        "    self.pool_stride = pool_stride\n",
        "    self.conv_block = nn.Sequential(\n",
        "        nn.Conv2d(3, 16, kernel_size = self.kernel_size, stride=self.stride,padding= self.padding),\n",
        "        nn.BatchNormr2d(16),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(self.pool, self.pool_stride)\n",
        "                  )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    forward(): 모델에서 실행되어야 하는 계산을 정의\n",
        "    backward 계산은 backward()를 이용하면 pytorch가 자동으로 해주므로 forward()만 정의하면 됨.\n",
        "\n",
        "    input을 넣어서 어떤 계산을 진행하여 output이 나올지 정의해줌.\n",
        "    '''\n",
        "    output= self.conv_block(x)\n",
        "    return output\n",
        "  \n",
        "#model = ConvBlock(...)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
